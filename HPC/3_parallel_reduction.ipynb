{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrGv2RHQPmdI",
        "outputId": "5a0e31d6-ef92-4f85-9f9f-7e9c5aa564f9"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/afnan47/cuda.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZKcBv0PQImN",
        "outputId": "db1f3ef3-9900-4fba-f5d7-2c104bfae660"
      },
      "outputs": [],
      "source": [
        "%load_ext nvcc_plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXtkUCowQZqd",
        "outputId": "d248be7c-63b9-4cc3-cd59-3a5962cea92a"
      },
      "outputs": [],
      "source": [
        "// WARNING: DO NOT COPY THIS CODE, INSTEAD DOWNLOAD IT TO AVOID ERRORS.\n",
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Kernel for parallel reduction using min operation\n",
        "__global__ void reduceMin(int* input, int* output, int size) {\n",
        "    __shared__ int sdata[BLOCK_SIZE];\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Load data into shared memory\n",
        "    if (i < size) {\n",
        "        sdata[tid] = input[i];\n",
        "    } else {\n",
        "        sdata[tid] = INT_MAX;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Perform reduction within each block\n",
        "    for (unsigned int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sdata[tid] = min(sdata[tid], sdata[tid + stride]);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write the result for this block to global memory\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for parallel reduction using max operation\n",
        "__global__ void reduceMax(int* input, int* output, int size) {\n",
        "    __shared__ int sdata[BLOCK_SIZE];\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Load data into shared memory\n",
        "    if (i < size) {\n",
        "        sdata[tid] = input[i];\n",
        "    } else {\n",
        "        sdata[tid] = INT_MIN;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Perform reduction within each block\n",
        "    for (unsigned int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sdata[tid] = max(sdata[tid], sdata[tid + stride]);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write the result for this block to global memory\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for parallel reduction using sum operation\n",
        "__global__ void reduceSum(int* input, int* output, int size) {\n",
        "    __shared__ int sdata[BLOCK_SIZE];\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Load data into shared memory\n",
        "    if (i < size) {\n",
        "        sdata[tid] = input[i];\n",
        "    } else {\n",
        "        sdata[tid] = 0;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Perform reduction within each block\n",
        "    for (unsigned int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sdata[tid] += sdata[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write the result for this block to global memory\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for parallel reduction using average operation\n",
        "__global__ void reduceAverage(int* input, float* output, int size) {\n",
        "    __shared__ float sdata[BLOCK_SIZE];\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Load data into shared memory\n",
        "    if (i < size) {\n",
        "        sdata[tid] = static_cast<float>(input[i]);\n",
        "    } else {\n",
        "        sdata[tid] = 0.0f;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Perform reduction within each block\n",
        "    for (unsigned int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sdata[tid] += sdata[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write the result for this block to global memory\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = sdata[0] / static_cast<float>(size);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Input array\n",
        "    const int array_size = 256;\n",
        "    int input[array_size];\n",
        "\n",
        "    // Initialize input array\n",
        "    for (int i = 0; i < array_size; ++i) {\n",
        "        input[i] = i + 1;\n",
        "    }\n",
        "\n",
        "    // Allocate device memory\n",
        "    int* d_input;\n",
        "    int* d_output_min;\n",
        "    int* d_output_max;\n",
        "    int* d_output_sum;\n",
        "    float* d_output_avg;\n",
        "    cudaMalloc((void**)&d_input, sizeof(int) * array_size);\n",
        "    cudaMalloc((void**)&d_output_min, sizeof(int) * array_size);\n",
        "    cudaMalloc((void**)&d_output_max, sizeof(int) * array_size);\n",
        "    cudaMalloc((void**)&d_output_sum, sizeof(int) * array_size);\n",
        "    cudaMalloc((void**)&d_output_avg, sizeof(float) * array_size);\n",
        "\n",
        "    // Copy input array to device memory\n",
        "    cudaMemcpy(d_input, input, sizeof(int) * array_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Determine the number of threads and blocks\n",
        "    int threads_per_block = BLOCK_SIZE;\n",
        "    int blocks_per_grid = (array_size + threads_per_block - 1) / threads_per_block;\n",
        "\n",
        "    // Launch the kernels for parallel reduction\n",
        "    reduceMin<<<blocks_per_grid, threads_per_block>>>(d_input, d_output_min, array_size);\n",
        "    reduceMax<<<blocks_per_grid, threads_per_block>>>(d_input, d_output_max, array_size);\n",
        "    reduceSum<<<blocks_per_grid, threads_per_block>>>(d_input, d_output_sum, array_size);\n",
        "    reduceAverage<<<blocks_per_grid, threads_per_block>>>(d_input, d_output_avg, array_size);\n",
        "\n",
        "    // Copy the results back to the host\n",
        "    int min_result, max_result, sum_result;\n",
        "    float avg_result;\n",
        "    cudaMemcpy(&min_result, d_output_min, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(&max_result, d_output_max, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(&sum_result, d_output_sum, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(&avg_result, d_output_avg, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print the results\n",
        "    printf(\"Minimum value: %d\\n\", min_result);\n",
        "    printf(\"Maximum value: %d\\n\", max_result);\n",
        "    printf(\"Sum: %d\\n\", sum_result);\n",
        "    printf(\"Average: %.2f\\n\", avg_result);\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output_min);\n",
        "    cudaFree(d_output_max);\n",
        "    cudaFree(d_output_sum);\n",
        "    cudaFree(d_output_avg);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
